# Quick test config for validating all Wandb graphs
model:
  max_seq_length: 128           # Reduced for faster training
  hidden_size: 64               # Smaller model
  num_layers: 2                 
  num_heads: 4                  
  dropout: 0.1                  
  training_mode: "diffusion"    
  vocab_size: 50257             
  mask_token_id: 50256          

# Advanced model settings (required by model.py)
advanced:
  attention_dropout: 0.1
  layer_norm_eps: 1e-5
  ignore_index: -100

diffusion:
  single_ratio_per_sequence: false    
  min_mask_ratio: 0.0                 
  max_mask_ratio: 1.0                 
  confidence_threshold: 0.8           
  max_loss_weight: 5.0               

training:
  batch_size: 64                     # Number of samples per batch (total_samples/batch_size = batches_per_epoch)
  gradient_accumulation_steps: 1     # Accumulate gradients over 4 batches before optimizer step (effective_batch_size = 8*4 = 32)

  num_epochs: 5                      # Total number of training epochs 
  save_every_n_steps: 10             # Save model every number of steps (regular checkpoint)
  save_every_n_epochs: 1             # Save every number of epochs (epoch-based checkpoint)  
  eval_every_n_steps: 50             # Validate and log every number of steps (saves best model if improved)
  logging_steps: 25                  # Log training metrics every number of steps (align with eval for clean wandb graphs; use 0 or 5)
  detailed_metrics_steps: 50         # Log research metrics every number of steps (align with eval for clean wandb graphs; use 0 or 5)
  attention_analysis_steps: 100      # Log advanced attention metrics every number of steps (align with eval for clean wandb graphs; use 0 or 5)

  learning_rate: 3e-4                # Peak learning rate for AdamW optimizer
  min_learning_rate: 3e-5            # Minimum learning rate after cosine decay (10% of peak)
  weight_decay: 0.1                  # L2 regularization strength (prevents overfitting)
  gradient_clip: 1.0                 # Maximum gradient norm (prevents exploding gradients)
  beta1: 0.9                         # AdamW momentum parameter (exponential decay rate for 1st moment)
  beta2: 0.95                        # AdamW momentum parameter (exponential decay rate for 2nd moment) 
  eps: 1e-8                          # AdamW numerical stability epsilon (prevents division by zero)
  warmup_ratio: 0.005                # Fraction of total steps for learning rate warmup (1% of training)

  warmup_start_factor: 0.001         # Start at 0.1% of target LR during warmup (instead of 0.0001%)  
  warmup_end_factor: 1.0             # Reach 100% of target LR after warmup period
  checkpoint_map_location: "cuda"    # Load checkpoints directly to GPU (avoids CPUâ†’GPU transfer)
  cleanup_checkpoints: true          # true = delete all checkpoints except best and latest
  perplexity_cap: 15.0               # Maximum loss value for perplexity calculation (prevents math.exp overflow)

data:
  num_workers: 8

sampling:
  num_steps: 10                      # Faster sampling
  temperature: 1.0                   
  top_k: 50                          
  top_p: 0.9                         
  confidence_threshold: 0.8          
  schedule_type: "cosine"            

monitoring:
  wandb:
    enabled: true
    project: "tdlm-quick-test"
    tags: ["quick-test", "all-graphs-working"]

reproducibility:
  seed: 42

enhanced_evaluation:         # Whether to include downstream task evaluation (HellaSwag, MMLU samples)
  include_downstream: true   # Set to false for faster evaluation focusing only on perplexity
  max_eval_batches: null     # Maximum number of batches to evaluate during final evaluation (null = evaluate all test batches and most accurate)